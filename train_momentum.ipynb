{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader as DataLoader\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "from matplotlib import pyplot as plt\n",
    "from melSpecDataset import MelSpecDataset\n",
    "import basic_model as net0\n",
    "import ModMusicRedNet as net1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set run choices\n",
    "loss_plot = True\n",
    "verbose = True\n",
    "epoch_save = False\n",
    "\n",
    "# set variables\n",
    "train_dir = './splitdata/training'\n",
    "val_dir = './splitdata/testing'\n",
    "num_epochs = 10\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "weight_decay = 1e-4\n",
    "rho = 0.9\n",
    "eps = 1e-06\n",
    "momentum = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcMeanStd ():\n",
    "    # Assuming MelSpecDataset is your dataset class and train_dir is your training directory\n",
    "    resize_size = (258, 128)\n",
    "    dataset = MelSpecDataset(train_dir, transform=Compose([Resize(resize_size), ToTensor()]))\n",
    "\n",
    "    # Create a DataLoader with the desired batch size\n",
    "    loader = DataLoader(dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "    # Variables to accumulate the sum and sum of squares\n",
    "    mean_sum = 0.0\n",
    "    sum_of_squares = 0.0\n",
    "    nb_samples = 0\n",
    "\n",
    "    # Loop through all the batches in the DataLoader\n",
    "    for images, _ in loader:\n",
    "        # Flatten the images to (batch_size, pixels)\n",
    "        images = images.view(images.size(0), -1)\n",
    "        # Sum up the mean and mean of squares\n",
    "        mean_sum += images.mean(1).sum(0)\n",
    "        sum_of_squares += (images ** 2).mean(1).sum(0)\n",
    "        # Count the total number of samples (images) processed\n",
    "        nb_samples += images.size(0)\n",
    "\n",
    "    # Calculate the mean and standard deviation\n",
    "    mean = mean_sum / nb_samples\n",
    "    # For std, we need to take the square root of the variance (average of the squared differences from the mean)\n",
    "    std = (sum_of_squares / nb_samples - mean ** 2) ** 0.5\n",
    "\n",
    "    # Convert to scalar for single-channel image\n",
    "    mean = mean.item()\n",
    "    std = std.item()\n",
    "\n",
    "    #print(f'Calculated mean: {mean}')\n",
    "    #print(f'Calculated std: {std}')\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loaded sucessfully\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#need to change based on model name\n",
    "#this calls the constructor of model class setting the choosen model for the run\n",
    "################################################################################\n",
    "#model = net0.GenreClassificationANN()\n",
    "model = net1.MusicClassNet()\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "################################################################################\n",
    "#dataloader\n",
    "mean, std = calcMeanStd()\n",
    "resize_size = (258, 128)\n",
    "transform = Compose([\n",
    "    Resize(resize_size),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=[mean], std=[std]) \n",
    "])\n",
    "\n",
    "#training\n",
    "train_dataset = MelSpecDataset(train_dir, transform)\n",
    "data_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#validation\n",
    "val_dataset = MelSpecDataset(val_dir, transform)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(\"data loaded sucessfully\")\n",
    "#model optimizer\n",
    "#optimizer = torch.optim.Adadelta(model.parameters(), lr=learning_rate, rho=rho, eps=eps, weight_decay=weight_decay)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "#model scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=6, verbose=True)\n",
    "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=4, gamma=0.1)\n",
    "\n",
    "\n",
    "# loss function\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "\n",
    "    epoch_losses = []\n",
    "    epoch_losses_val = []\n",
    "    n_batches_train = len(data_loader)\n",
    "    n_batches_val = len(val_loader)\n",
    "\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        batch_loss = 0\n",
    "        print(f'Epoch #{epoch}, Start Time: {datetime.datetime.now()}')\n",
    "\n",
    "        #training\n",
    "        model.train()\n",
    "        for melspecs, labels in data_loader:\n",
    "            \n",
    "            #print(melspecs)\n",
    "            #print(labels)\n",
    "            audios = melspecs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            print(\"before model\")\n",
    "            # calculate losses and call call model\n",
    "            output = model(audios)\n",
    "            print(\"ouput reached\")\n",
    "            # batch loss\n",
    "            loss = loss_function(output, labels)\n",
    "            #print(loss)\n",
    "            batch_loss += loss.item()\n",
    "            #print(batch_loss)\n",
    "\n",
    "            # backpropogation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # save epoch loss\n",
    "        epoch_losses += [batch_loss/n_batches_train]\n",
    "        #scheduler.step()\n",
    "        scheduler.step(epoch_losses[-1])\n",
    "        print(\"epoch reached\")\n",
    "        \n",
    "\n",
    "        #validation\n",
    "        loss_val = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for melspecs, labels in val_loader:\n",
    "                audios = melspecs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                output = model(audios)\n",
    "                #print(output)\n",
    "\n",
    "                # calculate losses\n",
    "                loss = loss_function(output, labels)\n",
    "                loss_val += loss.item()\n",
    "        \n",
    "        epoch_losses_val += [loss_val/n_batches_val]\n",
    "        #scheduler.step(epoch_losses_val[-1])\n",
    "        \n",
    "\n",
    "        if verbose:\n",
    "            print(f'Epoch: #{epoch}, Loss: {epoch_losses[epoch-1]}')\n",
    "            print(f'Epoch: #{epoch}, Val_Loss: {epoch_losses_val[epoch - 1]}')\n",
    "\n",
    "        if epoch_save:\n",
    "            model_folder_dir = './temp_models'\n",
    "            if not os.path.isdir(model_folder_dir):\n",
    "                os.mkdir(model_folder_dir)\n",
    "\n",
    "             # save temp model\n",
    "            try:\n",
    "                temp_model_path = model_folder_dir + '/' + str(datetime.datetime.now()) + '_epoch' + str(epoch) + '.pth'\n",
    "                torch.save(model.state_dict(), temp_model_path)\n",
    "                if verbose:\n",
    "                    print(f'Saved model for epoch {epoch} @{temp_model_path}')\n",
    "            except:\n",
    "                print('Epoch model save failed')\n",
    "\n",
    "    #save final model parameters   \n",
    "    timestamp = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "    torch.save(model.state_dict(), f'model_mod_{timestamp}.pth')\n",
    "\n",
    "    # save final loss plot\n",
    "    if not os.path.exists(\"./plots\"):\n",
    "        os.makedirs(\"./plots\")\n",
    "    timestamp = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "    #generate_loss_plot(epoch_losses, f'./plots/loss_plot_{timestamp}.png', show_plot=loss_plot)\n",
    "    generate_loss_plot_with_val(epoch_losses, epoch_losses_val, f'./plots/loss_plot_{timestamp}.png', show_plot=loss_plot)\n",
    "\n",
    "def generate_loss_plot(loss, file_loc, show_plot=False):\n",
    "    epochs = list(range(1, len(loss)+1))\n",
    "    plt.plot(epochs, loss)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Epoch vs Loss')\n",
    "    plt.savefig(file_loc)\n",
    "    if show_plot:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def generate_loss_plot_with_val(train_loss, val_loss, file_loc, show_plot=False): # loss plot with validation\n",
    "    epochs = list(range(1, len(train_loss)+1))\n",
    "    plt.plot(epochs, train_loss, label = \"Training Loss\")\n",
    "    plt.plot(epochs, val_loss, label= \"Validation Loss\")\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Epoch vs Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig(file_loc)\n",
    "    if show_plot:\n",
    "        plt.show()\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1, Start Time: 2023-12-05 15:58:29.027354\n",
      "before model\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (32x512 and 1024x256)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\saman\\VSCPython\\CMPE452_Proj\\train_momentum.ipynb Cell 7\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/saman/VSCPython/CMPE452_Proj/train_momentum.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/saman/VSCPython/CMPE452_Proj/train_momentum.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     train()\n",
      "\u001b[1;32mc:\\Users\\saman\\VSCPython\\CMPE452_Proj\\train_momentum.ipynb Cell 7\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/saman/VSCPython/CMPE452_Proj/train_momentum.ipynb#W5sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mbefore model\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/saman/VSCPython/CMPE452_Proj/train_momentum.ipynb#W5sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m# calculate losses and call call model\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/saman/VSCPython/CMPE452_Proj/train_momentum.ipynb#W5sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m output \u001b[39m=\u001b[39m model(audios)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/saman/VSCPython/CMPE452_Proj/train_momentum.ipynb#W5sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mouput reached\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/saman/VSCPython/CMPE452_Proj/train_momentum.ipynb#W5sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39m# batch loss\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\saman\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\saman\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\saman\\VSCPython\\CMPE452_Proj\\ModMusicRedNet.py:55\u001b[0m, in \u001b[0;36mMusicClassNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     53\u001b[0m x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdense_00(x))\n\u001b[0;32m     54\u001b[0m \u001b[39m#x = self.drop(x)\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdense_01(x))\n\u001b[0;32m     56\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdrop(x)\n\u001b[0;32m     57\u001b[0m x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdense_1(x))\n",
      "File \u001b[1;32mc:\\Users\\saman\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\saman\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\saman\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (32x512 and 1024x256)"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
